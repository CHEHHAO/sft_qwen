import os
import time
import logging
from flask import Flask, request, jsonify
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from utils.utils import classify_emotion
from config.config import API_INFERENCE_LOG_PATH, WEIGHT_MODEL_PATH, LORA_MODEL_PATH, DEVICE

LOG_PATH = API_INFERENCE_LOG_PATH
MODEL_PATH = WEIGHT_MODEL_PATH + "_response/final"
device = DEVICE

# === Êó•ÂøóÈÖçÁΩÆ ===
os.makedirs(os.path.dirname(LOG_PATH), exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format="[%(asctime)s] %(levelname)s: %(message)s",
    handlers=[
        logging.FileHandler(LOG_PATH, encoding="utf-8"),
        logging.StreamHandler()
    ]
)

# === ÂàùÂßãÂåñ Flask Â∫îÁî® ===
app = Flask(__name__)
logging.info("üöÄ Flask app initialized.")

# === Âä†ËΩΩÊ®°Âûã ===

logging.info(f"üîß Loading model from: {MODEL_PATH}")
logging.info(f"üñ•Ô∏è Using device: {device}")

tokenizer = AutoTokenizer.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True,
    local_files_only=True
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    attn_implementation="flash_attention_2",
    local_files_only=True
).to(device)
model.eval()

logging.info("‚úÖ Model and tokenizer loaded successfully.")

# === Ë∑ØÁî±ÂÆö‰πâ ===
@app.route("/predict", methods=["POST"])
def predict():
    start_time = time.time()
    data = request.get_json()
    text = data.get("text", "").strip()

    if not text:
        logging.warning("‚ö†Ô∏è Received empty input.")
        return jsonify({"error": "Missing input text"}), 400

    logging.info(f"üì® Received text: {text}")

    try:
        prediction = classify_emotion(text, model, tokenizer, device, cli=True)
        elapsed = round((time.time() - start_time) * 1000, 2)
        logging.info(f"‚úÖ Prediction: {prediction} | ‚è±Ô∏è Inference time: {elapsed} ms")

        return jsonify({
            "input": text,
            "prediction": prediction,
            "time_ms": elapsed
        })

    except Exception as e:
        logging.error(f"‚ùå Exception during prediction: {str(e)}")
        return jsonify({"error": str(e)}), 500

# === ÂêØÂä®ÊúçÂä° ===
if __name__ == "__main__":
    logging.info("üöÄ Starting Flask inference server...")
    app.run(host="0.0.0.0", port=8000)
